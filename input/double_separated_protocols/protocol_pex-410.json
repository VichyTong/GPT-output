[
    "<p>",
    "The proposed protocol consists of the following five stages, each comprising a number of steps: (a) formal problem definition (5 steps), (b) data processing (10 steps), (c) model architecture selection (10 steps), (d) risk calibration and uncertainty (3 steps), and (e) model generalisability evaluation (6 steps).",
    "In total we describe 34 steps with each step itself comprising a number of sub-steps.</p><p><strong>Formal problem definition</strong></p><p><strong>1.",
    "Evaluate the feasibility of a clinical use case:</strong> Identify a clinical use case involving adverse event prediction where sufficient clinical data are available.",
    "Formulate the research question through patient engagement, structured interviews with clinical teams and consultation of clinical guidelines and literature evidence.",
    "Map real life patient pathways associated with the clinical problem and ",
    "Evaluate whether the proposed research question will impact clinical practice as intended.",
    "If there is consensus between multiple clinical and patient experts on the feasibility of the use case, proceed.</p><p><strong>2.",
    "Obtain or derive ground truth labels.</strong> Identify an appropriate ground truth 'label' in the data for the model to predict that is as ",
    "Close to a gold standard diagnosis as possible.",
    "Internationally accepted proxy measures - such as \"Kidney Disease: Improving Global Outcomes (KDIGO)\" criteria<sup>24</sup> for AKI - may be used where obtaining gold standard labels is not feasible.</p><p><strong>3.",
    "Inclusion and exclusion criteria:</strong> Identify types of patients that should be excluded from research data, based on the particular clinical use case.",
    "Re",
    "Move all patient records that match the full patient exclusion criteria.",
    "For the remaining patient records, define exclusion criteria on a level of individual time steps and intervals that are to be excluded either entirely or only from evaluation, while permitted in model training.",
    "For example, when predicting future AKI, models should not be evaluated on a step-by-step basis while within an AKI episode, but there is still bene",
    "Fit in including these segments in model training since the models can learn about patterns of kidney injury progression and when it is likely to resolve or deteriorate further.",
    "Time intervals during which the patients are receiving renal replacement therapy (RRT) should be excluded both from training and testing splits.",
    "Surrogate entries are to be excluded in all cases (",
    "See Step 8).",
    "To exclude these identified time intervals, compute binary overlay <em>masks</em> associated with each step in each patient, where the value of 1 marks the particular time step as being included and the value of 0 marks the step as being excluded for purposes of loss and metrics computation.",
    "Note that, importantly, these time steps are not programmatically removed from the data, allowing the models to sequentially iterate over them and form memory representations of past clinical events correspond to these intervals.",
    "Similarly, these entries are factored in the historical feature representations in data preparation.</p><p><strong>4.",
    "Assess dataset quality:</strong> Produce a formal specification of each data type present in the research dataset.",
    "Conduct structured interviews with experts with knowledge of the data to list known issues.",
    "Compute distributions of recorded blood test values in the EHR; ",
    "Compare the computed distributions to known physiological ranges.",
    "Validate admission records based on the recorded length of stay; identify admissions recorded as being within other admissions.",
    "Compile a random sample of the data for a closer clinical assessment of label quality; involve several clinical experts and have each expert ",
    "Evaluate each labelled example.",
    "Maintain a spreadsheet or database of all label assessments.",
    "Compute the variance in label assessments between different experts.",
    "Compute the percentage of ground truth labels produced by the primary labelling ",
    "Process marked as incorrect by being outside of the variance of the experts.",
    "Conduct meetings to reach clinical consensus on whether the computed percentage exceeds tolerable levels of labelling error.",
    "If the error level is deemed to be acceptable, proceed.</p><p><strong>5.",
    "Select auxiliary prediction targets:</strong> Identify blood tests and vitals directly related to the clinical use case.",
    "Exclude rare types of measurements.",
    "Expose the values of these entries as auxiliary prediction targets, adding interpretability and helping to regularise developed ML models<sup>9</sup>.</p><p><strong>Data processing</strong></p><p><strong>6.",
    "Create data splits:</strong> Randomly assign each patient ID to one of the following data splits: <em>training</em>, <em>validation</em>, <em>calibration</em> and <em>test</em>.",
    "The minimum size of each ",
    "Split needs to be sufficient to derive valid statistical conclusions about the models and the data and should be based on an appropriate power calculation.",
    "In large datasets, assigning 80% of the data to the <em>training</em> split, 5% of the data to the <em>validation</em> split, 5% of the data to the <em>calibration</em> ",
    "Split and 10% of the data to the <em>test</em> ",
    "Feature filtering:</strong>",
    "Conduct interviews with clinical experts and the data experts that provided the research dataset to identify EHR entries that are not likely to be of value for the considered clinical use case.",
    "Compile a set of feature types that are not thought to be informative or are too specific to individual sites, if the dataset is compiled from multiple sites.",
    "Implement a script that removes the identified entries from the research dataset.",
    "Execute the script and produce a data version with those features removed.</p><p><strong>8.",
    "Produce a discrete sequential EHR data representation:</strong>",
    "First, define the length of the atomic discrete time window.",
    "This can be defined as 6h<sup>20</sup> or 4h or any time window consistent with the granularity of the available data and the clinical use case.",
    "Select a time window that is a divisor of 24h.",
    "The inconsistencies in sequentiality of EHR entries limit the utility of very short atomic time windows.",
    "For each patient in all data splits, represent each day as a ",
    "Sequence of unordered sets of events that ",
    "Take ",
    "Place within equal-width time windows as defined above.",
    "For each day, provide an additional set to represent all the entries for which the date is known, but the timestamp is not present in the EHR; refer to this as the <em>surrogate bucket</em>.",
    "Sort the event sets chronologically and append the surrogate bucket at the end of each daily sequence.",
    "In cases when there are no EHR entries corresponding to a particular time window of a particular day for a particular patient record, insert an empty set.",
    "Compute the corresponding time for each event set and ",
    "Maintain a ",
    "Record of which time window corresponds to inpatient versus outpatient clinics.",
    "Concatenate all of the daily sequences into a full longitudinal representation for each patient in chronological order.</p><p><strong>9.",
    "Shift EHR entries that were timestamped as preceding the events they encode:</strong> Based on an understanding of processes behind the EHR that were used to compile the research dataset, define a list of potentially useful entries that may have been entered out of order, prior to the time when they actually occurred.",
    "In the manuscript<sup>20</sup> this was the case with some of the diagnoses entries, which were timestamped at the beginning of admission even though the actual diagnosis might have been made at a slightly different time.",
    "To avoid leaking information from the future while retaining as much information as possible, ",
    "Move the entries in the sequential representation to a time window corresponding to the end of the current episode.</p><p><strong>10.",
    "Compute aggregate historical features:</strong> Generate a set of historical time windows over which to compute historical aggregate features.",
    "In practice, 48 hours can be used for shorter history, and longer historical trends can be captured by considering 6 months, 1 year or 5 years prior.",
    "Define a set of statistical functions to use for feature aggregation of numerical values.",
    "We recommend using count, mean, median, standard deviation, minimum value, maximum value, difference between the last observed value and the minimum/maximum, and average difference between subsequent steps.",
    "For each patient, for each time step, list all of the prior entries that fall within the specified list of historical time windows.",
    "Compute feature-specific historical lists for each feature index for that step.",
    "Apply each of the selected statistical functions for feature aggregation to each of the numerical feature lists for each of the time steps for each patient.",
    "For non-numerical features, ",
    "Record a binary flag for whether they were or were not present in the interval.",
    "Record the computed values as ",
    "Separate historical features associated with each steps, assigning them a unique feature index.",
    "Perform this for each of the historical windows under consideration.</p><p><strong>11.",
    "Compute manually-engineered features:</strong>&nbsp;Identify a set of manually-engineered features that may ",
    "Hold predictive value, based on a thorough literature review as well as domain knowledge provided by clinical experts.",
    "Examples ",
    "Include quantity ratios (e.g. ratio of blood urea nitrogen to serum creatinine), group features that correspond to sets of related EHR entries (e.g. a combined group feature derived from ICD-9 codes 250.42 and 250.4 as well as blood levels of haemoglobin A1c can help identify diabetic patients) and interaction terms.",
    "Implement functions that can be used to compute these features when applied to each step in the sequence.",
    "For each patient and each time step, compute the specified manually-engineered features.</p><p><strong>12.",
    "Vectorise the event sequence:</strong>",
    "For each numerical entry, define the following features: the actual numerical value if available, a binary indicator of whether the feature was present in a particular time window, and a categorical variable corresponding to whether the quantity is considered to be normal, low or high, based on physiological ranges.",
    "For all numerical, binary and categorical features, reserve a unique index.",
    "For numerical and binary entries, for each patient and each step provide the <em>(index, value)</em> pair.",
    "For categorical features, for each patient and each step compute the one-hot encoding<sup>42</sup>, representing each possible value as a ",
    "Separate binary feature.",
    "Convert all values to a floating point format, suitable for model development.</p><p><strong>13.",
    "Normalise the numerical features:</strong> To improve convergence speed<sup>43</sup>, normalise the in",
    "Put data features to be within the unit range 0-1.",
    "First, for every in",
    "Put feature compute the 1st and 99th percentile values on the training set.",
    "Using percentiles is preferable to using the minimum and maximum values, as these are more prone to data entry errors, resulting in physiologically implausible minimum/maximum entries in EHR data.",
    "For each value of each feature in each step for each patient, clip the value according to these percentiles as <em>value</em>=clip(<em>value</em>), where clip(<em>value</em>)=min(max(<em>value</em>, <em>percentile</em><sub><em>1</em></sub>), <em>percentile</em><sub><em>99</em></sub>).",
    "Proceed to normalise each of the clipped values by <em>value</em>=normalise(<em>value</em>), where normalise(<em>value</em>) = (<em>value</em> - <em>percentile</em><sub><em>1</em></sub>) /",
    "(<em>percentile</em><sub><em>99</em></sub> - <em>percentile</em><sub><em>1</em></sub>).</p><p><strong>14.",
    "Compute model targets:</strong> Define a list of future time windows for which to predict the expected primary and auxiliary target values, based on plausible clinical assumptions about the earliest possible time at which a condition might be predictable in certain clinical cases.",
    "For example, for AKI prediction<sup>20</sup>, we considered future predictions up to 72 hours ahead of time, though we present the main results of future AKI prediction up to 48 hours ahead of AKI onset, based on considerations around model performance and the predictability of AKI.",
    "In that case, the future prediction time window list was defined as: 6, 12, 18, 24, 30, 36, 42, and 48 hours.",
    "For each patient and each step and for each future time window and each of the prediction tasks, compute the corresponding observed future label to be used for model development and evaluation.",
    "For binary targets the future label is to be 1 if the condition had occurred within the window and 0 if the condition did not occur within the window, for the given step.",
    "For numerical targets, define the statistics of interest and compute them across the future time windows for each of the numerical targets at each step.</p><p><strong>15.",
    "Implement clinically relevant performance metrics:</strong>",
    "Define a set of relevant metrics both for the primary use case as well as the auxiliary prediction targets.",
    "For each, define separately a) model development metrics to be used for model architecture selection and b) final model evaluation metrics.",
    "For binary (non-numeric) targets, use area under the precision-recall curve (PR AUC) and the area under the receiver operating characteristic curve (ROC AUC) in model development.",
    "For final model evaluation, compute for each future prediction window the sensitivity for multiple levels of precision that correspond to different model operating points<sup>44</sup>.",
    "For numerical targets, compute the mean squared error (MSE) during model development.",
    "For final model evaluation, compute the percentage of correctly predicted substantial increases or decreases in value.",
    "Implement the relevant binary masks for step sub-selection according to the relevant stepwise inclusion and exclusion criteria, as per Step 3.</p><p><strong>Model architecture selection</strong></p><p><strong>16.",
    "Compute standard statistical baseline models:</strong> Identify a set of statistical techniques commonly used in previous publications and protocols for a given clinical targets.",
    "These can ",
    "Include prediction models (e.g. Logistic Regression, Gradient Boosted Trees) and data processing techniques (e.g. Principal Component Analysis, Multiple Imputation).",
    "Estimate the parameters for those models and compute predictions for a given target.",
    "Estimate the performance for the models and use them for comparison during step 21.</p><p><strong>17.",
    "Embed the inputs into a continuous feature space:</strong>",
    "For each timestep, ",
    "Transform the high-dimensional and sparse in",
    "Put features into a lower-dimensional continuous representation (i.e. embedding), that will be later used as an in",
    "First, compute ",
    "Separate embeddings for ",
    "Put features: discrete features, continuous features, sequential and historical aggregations.",
    "Next, ",
    "Combine the resulting embedding vectors by concatenating them.",
    "Use a deep multilayer perceptron with residual connections and rectified-linear (ReLU) activations to ",
    "Capture the non-linear representation.",
    "Use L<sub>1</sub> regularisation on the embedding parameters to prevent overfitting and to ",
    "Ensure that the model focuses on the most salient features.",
    "Implement an optional reconstruction path with a reconstruction loss at each step and a framework that supports the use of autoencoders (AEs) and variational autoencoders (VAEs).</p><p><strong>18.",
    "Implement a deep architecture on top of in",
    "Put embeddings:</strong>",
    "Implement a recurrent and convolutional deep learning framework that sequentially takes the in",
    "Put embeddings as its in",
    "Make the frameworks configurable with respect to recurrent cell types and their parameters, as well as different types of convolutional kernels and architectures.",
    "Implement a batching and queuing ",
    "Process that provides the in",
    "Put embeddings sequentially to deep learning models.</p><p><strong>19.",
    "Set up the model optimiser:</strong> Using the out",
    "Put modelled ",
    "Sequence and the ground truth sequence, compute a scalar loss value for each time step.",
    "Next, compute scalar losses for each auxiliary task in the same fashion.",
    "The loss function is dependent on the type of modelled target.",
    "In Toma\u0161ev et al<sup>20</sup> cross-entropy loss function (Bernoulli log-likelihood) was used for a binary adverse event classification, and L1/L2 losses for the auxiliary laboratory test regression.",
    "Optionally, re-weight the loss to ac",
    "Count for skewed target distribution.",
    "Define a composite loss as a weighted sum of primary and auxiliary losses.",
    "Add the regularisation L1/L2 loss dependent on a subset of model weights that used to compute in",
    "Put embeddings.",
    "Initialise all the weights according to Xavier initialisation<sup>45</sup>.",
    "Use the computed loss alongside a mini-batch Adam<sup>46</sup> optimiser to iteratively adapt the weights of the neural network architecture, based on the computed gradients of the loss.",
    "Train using exponential learning rate decay on the training data ",
    "Split until convergence.</p><p><strong>20.",
    "Test the validity of the implementation:</strong> Implement unit tests for all standalone components in the software for model implementation, as well as all components in data processing scripts.",
    "Resolve any outstanding issues and repeat the ",
    "Process until full code coverage is reached and no known issues are left unfixed.",
    "Implement integration and regression tests for the distributed data processing pipeline, as well as the model experimental framework.",
    "Resolve any issues that appear at the integration testing stage that were not previously identified in the unit testing stage.",
    "Repeat the ",
    "Process until no outstanding issues are left and the code is considered reliable.</p><p><strong>21.",
    "Run an iterative ",
    "Sequence of hyperparameter sweeps:</strong> Decide on a desired level of predictive model performance that needs to be reached in order to imply potential clinical applicability.",
    "Define a set of potentially promising parameter values for each of the parameters in the model that is set to be configurable.",
    "Refer to this set as the set of hyperparameters in model development (",
    "See Table 7 for a list of hyperparameters tested in the manuscript<sup>20</sup>).",
    "Repeatedly ",
    "Perform hyperparameter sweeps aiming to establish good hyperparameter combinations, until a desired level of performance is reached or the maximum amount of time or resources is consumed.",
    "If the target performance is not reached, revisit and expand earlier steps on data processing and model implementation.",
    "In each hyperparameter sweep, formulate a hypothesis for which model architectural changes are likely to lead to performance improvements based on expert ML knowledge.",
    "In each sweep, ",
    "Generate a set of random combinations of hyperparameters corresponding to the current working hypothesis.",
    "For each of the hyperparameter combinations, train a model on the training set to ",
    "Obtain a set of models.",
    "Evaluate all trained models on the validation set by computing the clinically relevant model development metrics.",
    "Select models based primarily on the clinically relevant metrics for the primary prediction task.",
    "Whilst using PR AUC and ROC AUC to ",
    "Select candidate hyperparameter configurations, prefer configurations that improve on both ROC AUC and PR AUC performance.",
    "When comparing configurations where configuration A achieves a higher PR AUC and configuration B a higher ROC AUC, prefer configuration A. Select one or more of the most promising hyperparameter configurations to ",
    "Move into the next sweep when exploring additional hypotheses.",
    "Repeating this iterative ",
    "Process over time should result in improving the model performance based on a refinement of ",
    "Design choices.",
    "Select the best performing configuration at the end of this ",
    "Process as the final model architecture at this stage.</p><p><strong>22.",
    "Perform an ablation study:</strong>",
    "Take the final model architecture from the previous step and define a set of components to systematically ",
    "Re",
    "Move to ",
    "Assess their contribution to the overall performance, i.e. to ablate.",
    "These will be components of the model that can either be simplified or removed.",
    "For example, this can ",
    "Include the number of stacked layers in the deep model architecture, additional feature in",
    "Put types like the historical aggregate features, regularisation approaches, auxiliary prediction tasks, layer width etc.",
    "For each ablation component, train a predictive model with the components removed or simplified on the training set.",
    "Next, ",
    "Evaluate each of the ablated models on the validation set, by computing the model development metrics.",
    "To test for statistical significance, train a collection of baseline and ablation models, each with sampled initial parameters, and ",
    "Calculate confidence intervals on the average performance.",
    "If any of the ablated models ",
    "Perform at least as well as the more complex final model at the end of the previous step, redefine the final model to be the best performing of the ablated models; reducing the model complexity.",
    "Repeat this ",
    "Process until the model can no longer be simplified without loss of performance.</p><p><strong>23.",
    "Compute feature saliency:</strong> Perform occlusion analysis by evaluating the ",
    "Change in model risk when each feature is individually occluded, i.e. is removed or set as missing.",
    "The occluded value should equate to the value used during training when a feature is not present; for sparse feature sets this is simply zero.",
    "For non-sparse data with no missing values, the occluded value could be a mean-value for the given feature, or sampled from the marginal distribution of values for this feature<sup>47</sup>.",
    "By monitoring the ",
    "Change in risk for each feature present versus not present, sampled over many time steps and patients, we can compute the average effect on the risk per feature.",
    "This can be used to infer the saliency of features, and the direction of effect.",
    "Unlike metrics which ",
    "Determine saliency based upon the gradient of the risk with respect to each input<sup>48</sup>, occlusion analysis is robust to different in",
    "Put features having different scales.</p><p><strong>24.",
    "Failure case analysis:</strong> Compute patient-level and admission-level metrics for all patients and admissions in the validation set.",
    "Compile a set of representative success and failure cases of predictions made by the model, as well as a set of best success and worst failure cases made by the model.",
    "Assess the failure cases by having them reviewed by a set of clinical experts.",
    "Examine feature saliency at the point in time when incorrect predictions were made and investigate the root cause of the issues.",
    "Maintain a spreadsheet or a database of these assessments and at the end of the evaluation, establish if there exist common causes of model failures.",
    "Assemble ML experts and clinical experts to propose potential solutions to ",
    "Reduce the number of model failures.",
    "If potential solutions are identified and judged as worth investigating, identify previous steps in the protocol that need adjusting and ",
    "Re",
    "Turn to those steps.",
    "Repeat the entire protocol from those steps onwards, with the proposed adjustments.",
    "When reaching this step again, re",
    "Assess and decide on whether to proceed.</p><p><strong>25.",
    "Define the final model architecture:</strong>",
    "Define the resulting model architecture as final and do not revisit any of the previous steps at this point.",
    "Use the fixed set of parameters corresponding to this model to compute the predictions for all time steps in all patients for each data split.</p><p><strong>Risk calibration and uncertainty</strong></p><p><strong>26.",
    "Train an ensemble of models for prediction uncertainty:</strong> To ",
    "Quantify the uncertainty of model predictions train an ensemble of multiple models with a fixed set of hyperparameters (corresponding to the final model) but different initial seeds, similar to Defauw et al<sup>49</sup>.",
    "To get the uncertainty ranges, ",
    "Trim distribution tails depending on the desired level of confidence.</p><p><strong>27.",
    "Compute the uncertainty estimates:</strong>",
    "To gauge uncertainty on a trained model's performance, ",
    "Calculate confidence intervals of performance metrics using bootstrapping",
    ".",
    "First, sample the entire validation and test dataset with replacement (e.g. for 95% confidence intervals, ",
    "Take 200 samples).",
    "Resample entire patient history to conform with bootstrapping assumption of resampling independent events.",
    "Next, compute the pivot bootstrap estimator<sup>50</sup> using resampled values.</p><p><strong>28.",
    "Re-calibrate the risk model:</strong> Use the previously computed calibration set to ",
    "Align the predicted values with the underlying probability of the adverse event occurring at a given time step.",
    "Fit an isotonic regression<sup>51</sup> model on the model predictions against the target variable on the calibration set.",
    "Assess the quality of the calibration by comparing uncalibrated predictions to recalibrated ones using Brier score<sup>52</sup> and reliability plots<sup>53</sup>.</p><p><strong>Model generalisability evaluation</strong></p><p><strong>29.",
    "Establish a risk score thres",
    "Hold for positive alerts:</strong> Performance metrics are dependent on the choice of an operating point.",
    "Evaluate precision and specificity for each time step and compute sensitivity for each individual AKI episode for each possible operating point on the validation set.",
    "Use that to ",
    "Examine the trade-off between increasing precision and decreasing sensitivity.",
    "Choose a specific operating point for a deployment strategy.</p><p><strong>30.",
    "Analyse model performance across sub-populations:</strong> To identify types of patients or events where the model is applicable, define <em>a priori</em> a set of clinical subpopulations relevant to the modelled target.",
    "These can ",
    "Include demographic (age, gender, ethnicity, geography) and medical characteristics.",
    "In the manuscript<sup>20</sup> these included patients with chronic kidney injury (CKD), diabetes, or admissions resulting in ICU transfer.",
    "Next, identify patients, admissions or individual time steps that fall into each type.",
    "Finally, compute the performance on each subpopulation.</p><p><strong>31.",
    "Quantify the expected daily alert rate:</strong>",
    "Align all the predictions for all of the patient sequences from the test set in time.",
    "For each day in the longitudinal test set, compute the percentage of inpatients on that day where the model produced a true positive alert, the percentage of cases where the model produced a false positive alert without having provided a true positive alert, and the percentage of cases where the model did not produce any alerts.",
    "Compute the mean daily alert rate across all days in the longitudinal set for the total percentage of alerts as well as the true and false positive alerts.",
    "Report this metric to inform of the likely resource burden in future prospective evaluation.</p><p><strong>32.",
    "Evaluate model generalisability on future unseen data (Figure 2):</strong> Choose a point in time t<sub>P</sub> such that approximately 80% of data entries occur prior to t<sub>P</sub> and approximately 20% occur after t<sub>P</sub>.",
    "Create the same data splits as described in step 6.",
    "Train a model using the final architecture determined in step 25 using only data entries that occurred prior to t<sub>P</sub> from the <em>training</em> split.",
    "Generate model predictions for the <em>test</em> split.",
    "Generate 95% confidence intervals of PR AUC for predictions made prior to t<sub>P</sub> and for predictions made subsequent to t<sub>P</sub>.",
    "Compare confidence intervals to ",
    "Determine if model performance on future unseen data is comparable to performance on historic unseen data.&nbsp;</p><p><strong>33.",
    "Evaluate model generalisability in simulated cross-site deployments (Figure 3):</strong> Choose a ",
    "Split in hospital sites such that approximately 80% of patient admissions occur at sites in group <em>A</em> and approximately 20% occur at sites in group <em>B</em>.",
    "Create the same data splits as described in step 6.",
    "Train a model using the final architecture determined in step 25 using the <em>training</em> split, excluding data entries from admissions at sites in group <em>B</em>.",
    "Run inference to ",
    "Generate model predictions for the <em>test</em> split.",
    "Generate 95% confidence intervals of PR AUC for predictions made during admissions at sites in group <em>A</em>",
    "and for admissions at sites in group <em>B</em>.",
    "Compare confidence intervals to ",
    "Determine if model performance for unseen sites is comparable to performance for sites used during training.</p><p><strong>34.",
    "Prospective evaluation:</strong> Prospectively ",
    "Evaluate the performance of the model in a real-world clinical environment.",
    "Initially, observational studies should seek to define (i) the feasibility of ingestion and processing of data, and the formulation and the delivery of predictions to clinicians in real time, (ii) how performance of the model is impacted by real-time deployment in specific clinical settings, (iii) the possible clinical and operational impacts of algorithm predictions on care delivered to patients, and (iv) when and how model predictions might best be presented to clinicians.",
    "Robust interventional studies with rigorous statistical methodology and analysis should then define the clinical, operational and economic impacts of deployment.",
    "Whilst detailing the optimal ",
    "Design of such evaluations falls outside the scope of this protocol, their conduct will be essential in demonstrating safety prior to widesp",
    "Read clinical use<sup>54</sup>.</p>"
]
[
    "\n<p><strong>1. Split a dataset randomly for a derivation and validation set</strong></p><p>Only the derivation set was used to estimate latent variables at the population level. This set was also used later for training set of a prediction model. This would make the model blinded to the distribution of weights for feature representation of any external validation sets.</p><p><br></p><p><strong>2. Choose resampling and dimensional reduction methods</strong></p><p>We made the rsdr 0.1.0 (an R package) that allows future investigators to conduct a principal component (PC) analysis or singular value decomposition using resampling methods, as described in this protocol. Instead of computing singular values by bootstrapping, as an example, we computed PCs by <em>k</em>-fold cross-validation for reasons of simplicity considering a simpler theoretical framework and an achievable computational capacity. To compute PCs by <em>k</em>-fold cross-validation, each of <em>β</em><sub><em>l</em></sub>, <em>μ</em><sub><em>j</em></sub>, and <em>σ</em><sub><em>j</em></sub>&nbsp;was inferred from the derivation set only, of which a <em>(K-k</em><sub><em>m</em></sub><em>)/K</em>&nbsp;part of <em>n</em>&nbsp;instances for <em>m=[1,2,⋯,K]</em>&nbsp;(equation in Figure",
    {
        "title": "1)",
        "body": [
            "was used each time to compute the variance.</p><p><br></p><p><strong>3. Standardize each variable with variable-wise average and standard deviation</strong></p><p>For every subset of resampling, an <em>X</em> matrix was constructed of <em>n×p</em> dimensions for <em>i=[1,2,⋯,n]</em> instances and <em>j=[1,2,⋯,p]</em> candidate predictors. Each vector was standardized with a column-wise <em>μ</em><sub><em>j</em></sub> mean and <em>σ</em><sub><em>j</em></sub> standard deviation of all instances for each candidate predictor.</p><p><br></p><p><strong>4. Map from higher to lower dimension by finding weights that maximize variances of new dimensions</strong></p><p>For every subset of resampling, we mapped each vector <em>x</em><sub><em>(i)</em></sub><em> </em>∈<em> X</em> onto a new vector of PC scores <em>t</em><sub><em>l(i)</em></sub><em> = x</em><sub><em>(i)</em></sub><em> ∙ β</em><sub><em>l</em></sub> for <em>l=[1,2,⋯,q]</em> by a matrix <em>β</em> of weight vectors where <em>q</em> ranged up to <em>p</em>. Mapping was used to find estimates of weight vectors that maximized the variance (equation in Figure",
            {
                "title": "1).",
                "body": "The <em>l</em><sup><em>th</em></sup> PC was calculated by subtracting the <em>l</em><sup><em>th</em></sup><em>-1</em> PC from <em>X</em>, then finding the estimate of the <em>l</em><sup><em>th</em></sup> PC as <em>l</em><sup><em>th</em></sup><em>-1</em> PC.</p><p><br></p><p><strong>5. Estimate</strong> <strong>variable-wise average and standard deviation</strong> <strong>and weights of the transformation at population level</strong></p><p>An estimate of the weight vector <em>β</em><sub><em>l</em></sub> was calculated by averaging <em>β</em><sub><em>l</em></sub>, <em>μ</em><sub><em>j</em></sub>, and <em>σ</em><sub><em>j</em></sub> from all <em>K=10</em> of <em>(K-k</em><sub><em>m</em></sub><em>)/K</em> parts. The eigenvalue of the matrix is commonly known for <em>X</em><sup><em>T</em></sup><em>X</em>, which achieves the maximum variance by <em>β</em> as the eigenvector. For each PC, one can find some original variables that are represented by a PC by filtering those with minimum absolute number of estimated weights of the transformation for that PC.</p><p><br></p><p><strong>6. Apply the estimated values to standardize and transform original variables into new dimensions</strong></p><p>Each of original variables in either derivation or validation set was standardized by subtracting it with the variable-wise estimated average, and subsequently by dividing it with the variable-wise estimated standard deviation. All of the standardized variables were mapped to each of PCs by multiplying each of these variables with the estimated weights. A dot product, which is a PC, was a sum of all the multiplication results.</p><p><br></p><p><strong>7. Select a particular number of new dimensions with highest proportions of variance explained</strong></p><p>This step is optional for predictive modeling. The recommended number of sample size in relative to the number of candidate predictors was computed for a specific algorithm (e.g. 200 events per variable for random forest).<sup>12</sup> Maximum number of candidate predictors was calculated by dividing the number of events with that number. The PCs were sorted by proportion of variance explained from the highest to the lowest. We selected top PCs as many as the maximum number of candidate predictors. Only top PCs were used for predictive modeling.</p>"
            }
        ]
    }
]